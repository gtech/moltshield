# Models

hf:moonshotai/Kimi-K2.5

hf:zai-org/GLM-4.7

hf:deepseek-ai/DeepSeek-R1-0528

hf:deepseek-ai/DeepSeek-V3-0324

hf:deepseek-ai/DeepSeek-V3.1

hf:deepseek-ai/DeepSeek-V3.1-Terminus

hf:deepseek-ai/DeepSeek-V3.2

hf:meta-llama/Llama-3.3-70B-Instruct

hf:MiniMaxAI/MiniMax-M2

hf:MiniMaxAI/MiniMax-M2.1

hf:moonshotai/Kimi-K2-Instruct-0905

hf:moonshotai/Kimi-K2-Thinking

hf:openai/gpt-oss-120b

hf:Qwen/Qwen3-235B-A22B-Instruct-2507

hf:Qwen/Qwen3-Coder-480B-A35B-Instruct

hf:Qwen/Qwen3-VL-235B-A22B-Instruct

hf:zai-org/GLM-4.6

hf:deepseek-ai/DeepSeek-V3

hf:Qwen/Qwen3-235B-A22B-Thinking-2507

# LoRA Models
What's a LoRA?
Low-rank adapters — called "LoRAs" — are small, efficient fine-tunes that run on top of existing models. They can modify a model to be much more effective at specific tasks.

We support LoRAs for the following base models:

meta-llama/Llama-3.2-1B-Instruct
meta-llama/Llama-3.2-3B-Instruct
meta-llama/Meta-Llama-3.1-8B-Instruct
meta-llama/Meta-Llama-3.1-70B-Instruct

# Embedding models:

hf:nomic-ai/nomic-embed-text-v1.5